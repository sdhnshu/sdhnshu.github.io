<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Sudhanshu Passi - Feature Engineering with Random Forests (Part 1)</title><meta name=description content="Sudhanshu Passi's personal website"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><link rel=stylesheet href=https://www.sdhnshu.com/css/bootstrap.min.css><link href="https://fonts.googleapis.com/css2?family=Lato&family=Merriweather:wght@300&display=swap" rel=stylesheet><link rel=stylesheet href=https://www.sdhnshu.com/css/font-awesome.min.css><link rel=stylesheet href=https://www.sdhnshu.com/css/owl.carousel.css><link rel=stylesheet href=https://www.sdhnshu.com/css/owl.theme.css><link href=https://www.sdhnshu.com/css/style.violet.css rel=stylesheet id=theme-stylesheet><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/atom-one-dark.min.css><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link href=https://www.sdhnshu.com/css/custom.css rel=stylesheet><link rel="shortcut icon" href=https://www.sdhnshu.com/img/favicon.ico><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-173272544-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div id=all><div class=container-fluid><div class="row row-offcanvas row-offcanvas-left"><div id=sidebar class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas"><div class=sidebar-content><h1 class=sidebar-heading><a href=https://www.sdhnshu.com/>Sudhanshu Passi</a></h1><ul class=sidebar-menu><li><a href=https://www.sdhnshu.com/showcase/>Showcase</a></li><li><a href=https://www.sdhnshu.com/experiments/>Experiments</a></li><li><a href=https://www.sdhnshu.com/about/>About</a></li></ul><p class=social><a href=https://github.com/sdhnshu data-animate-hover=pulse class=external><i class="fa fa-github"></i></a><a href=mailto:sudhanshupassi@gmail.com data-animate-hover=pulse class=email><i class="fa fa-envelope"></i></a><a href=https://www.linkedin.com/in/sdhnshu/ data-animate-hover=pulse class=external><i class="fa fa-linkedin"></i></a><a href=https://twitter.com/Sudhanshupassi data-animate-hover=pulse class="external twitter"><i class="fa fa-twitter"></i></a><a href=https://medium.com/@sdhnshu data-animate-hover=pulse class=external><i class="fa fa-medium"></i></a></p><div class=copyright><p class=credit>&copy; 2020 Sudhanshu Passi</p></div></div></div><div class="col-xs-12 col-sm-8 col-md-9 content-column white-background"><div class="small-navbar visible-xs"><button type=button data-toggle=offcanvas class="btn btn-ghost pull-left"> <i class="fa fa-align-left"></i>Menu</button><h1 class=small-navbar-heading><a href=https://www.sdhnshu.com/>Sudhanshu Passi</a></h1></div><div class=row><div class=col-lg-11><div class=content-column-content><h1>Feature Engineering with Random Forests (Part 1)</h1><i><p class=timestamp>Last updated Apr 29, 2020</p></i><p>Learn how to leverage Random Forests to do Feature Engineering and more.</p><ul><li>Originally published on <a href=https://medium.com/@sdhnshu/feature-engineering-with-random-forests-part1-601c66dfb09b>Medium</a></li><li>Grab the code from <a href=https://github.com/sdhnshu/Random-forests-and-Feature-Engineering>Github</a></li><li>Link to <a href=https://www.sdhnshu.com/showcase/random-forests-2>Part 2</a></li></ul><h3 id=introduction>Introduction</h3><p>Making an educated guess on what price should you sell a product is not everyone’s cup of tea. It needs a good amount of domain knowledge and at least some experience. And if we are giving this job to an algorithm, it makes it even more complicated.</p><blockquote><p><em>An algorithm is computationally scalable but logically bound.</em></p></blockquote><p>Unless you set these bounds the right way, you might not get the best output from it. This is what we are going to look at in this post.</p><p>We’ll go through the first 7 lectures of the <a href=http://course18.fast.ai/lessonsml1/lesson1.html>Fast.ai machine learning</a> course and learn how to predict the price of a bulldozer (<code>SalePrice</code>) using the <a href=https://www.kaggle.com/c/bluebook-for-bulldozers/data>data available</a> about its manufacturing and usage. In this post, we&rsquo;ll cover the random forests algorithm and feature engineering in the <a href=https://www.sdhnshu.com/showcase/random-forests-2>part 2</a>.</p><p><img src=https://miro.medium.com/max/573/0*easpKibgbYJgCSG8.png alt=img></p><h3 id=why-random-forests>Why Random Forests?</h3><p>Logically speaking, there are very few constraints in the Random Forests algorithm. Thus when we use it, we are introducing no biases from our end. And because it is simple, it is highly interpretable, unlike the black box algorithms that lack this feature.</p><blockquote><p><em>It takes the simplicity of a decision tree but generalizes it over a forest over the dataset to give us an exploratory tool that we can use as a flashlight in the dark.</em></p></blockquote><p>If you want to follow along, the jupyter notebook used in this post is available at the <a href=https://github.com/sdhnshu/Random-forests-and-Feature-Engineering>github repo</a> along with the Fast.ai library (need a local copy of v0.7, because it has been upgraded with major changes) and an environment file to set up an Anaconda environment. Now that you have everything ready, let us jump right in.</p><hr><h3 id=preprocessing>Preprocessing</h3><p>If you are new to Kaggle competitions, here are some <a href=https://www.kaggle.com/getting-started/44997>handy first steps</a>. If not, make sure you have gone through the <a href=https://www.kaggle.com/c/bluebook-for-bulldozers/overview>overview</a> and start looking at the data.</p><p><img src=https://miro.medium.com/max/573/0*xuijDvckOqpBhTqn.png alt=img>
<img src=https://miro.medium.com/max/573/0*s-G2Z18Bb5LGTYNd.png alt=img></p><p>By eyeballing, we can see that there are:</p><ul><li>Numerical features — measurement or count (<code>SalePrice</code>, <code>BladeWidth</code>)</li><li>Categorical features — ordinal and nominal (<code>TrackType</code>, <code>EnclosureType</code>, <code>ProductSize</code>, <code>UsageBand</code>)</li><li>DateTime features (<code>SaleDate</code>)</li><li>Ids (<code>MachineId</code>, <code>ModelId</code>)</li></ul><p>Download the <a href=https://www.kaggle.com/c/bluebook-for-bulldozers/data>data</a> locally in a folder ‘bluebook-for-bulldozers’ and extract the Train.zip.</p><p>Then let us look into some preprocessing techniques like:</p><ul><li>Log transform</li><li>Extracting data from DateTime</li><li>Handling categorical features</li><li>Imputing missing values</li></ul><h4 id=i-log-transform>i. Log Transform</h4><p>If you see the <a href=https://www.kaggle.com/c/bluebook-for-bulldozers/overview/evaluation>Evaluation metrics</a>, they want the Root Mean Squared Log Error.</p><blockquote><p><em>RMSE: standard deviation of our model’s prediction errors</em></p></blockquote><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>rmse</span>(actual, pred):
    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sqrt(np<span style=color:#f92672>.</span>mean((actual<span style=color:#f92672>-</span>pred)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>))
</code></pre></div><p>This is common for a feature like a price. They are more interested if you missed by 10% than if you missed by $10. If it is $10,000 item and you are off by a $1000 or if it is a $100 item and you are off by $10, they would be considered as equivalent issues.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>df_raw <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#39;bluebook-for-bulldozers/Train.csv&#39;</span>,
                     parse_dates<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;saledate&#39;</span>])
df_raw<span style=color:#f92672>.</span>SalePrice <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>log(df_raw<span style=color:#f92672>.</span>SalePrice)
</code></pre></div><p>Taking a log brings the data closer to a normal distribution. So, if the prediction errors are normally distributed, around 95% of the prediction errors would be in ±2*RMSE. It also decreases the effect of any outliers in the data. RMSE is the metric we’ll mathematically optimize and try to bring it down to 0.</p><p><img src=https://miro.medium.com/max/338/0*x_XL8FNSnnBOagEe.png alt=img></p><h4 id=ii-extracting-data-from-datetime>ii. Extracting Data from DateTime</h4><p>If some of your features are DateTime varaiables, we can decompose it into some useful features like — was it a weekend, what year was it, etc. Such engineering is often neglected but is very useful to extract the temporal information from the data. At the end of the post, we’ll also see how to remove any kind of temporal dependency from the data. This helps the model to generalize across time and will help us predict the <code>SalePrice</code> for the next month.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> fastai.structured <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>
<span style=color:#f92672>from</span> fastai.imports <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>add_datepart(df_raw, <span style=color:#e6db74>&#39;saledate&#39;</span>)
</code></pre></div><p><a href=https://github.com/fastai/fastai/blob/775dcd03f49f4f6385f19b503f954468b83afbea/old/fastai/structured.py#L70-L124>add_datepart:</a> Converts a datetime into <code>Year</code>, <code>Month</code>, <code>Week</code>, <code>Day</code>, <code>Dayofweek</code>, <code>Dayofyear</code>, <code>Is_month_end</code>, <code>Is_month_start</code>, <code>Is_quarter_end</code>, <code>Is_quarter_start</code>, <code>Is_year_end</code>, <code>Is_year_start</code> (and <code>Hour</code>, <code>Minute</code>, <code>Second</code>)</p><p><img src=https://miro.medium.com/max/182/0*VBpJuWcoh_FNtBo4.png alt=img>
<img src=https://miro.medium.com/max/573/0*4K_9QfdrE82tANuX.png alt=img></p><h4 id=iii-handling-categorical-features>iii. Handling Categorical Features</h4><p>Pandas can handle categorical features by maintaining a number:string hash.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>train_cats(df_raw)
</code></pre></div><p><a href=https://github.com/fastai/fastai/blob/775dcd03f49f4f6385f19b503f954468b83afbea/old/fastai/structured.py#L128-L153>train_cats:</a> This function from the Fast.ai library converts any columns of strings to a column of categorical values, in-place. It also fills empty/Nan values with code=-1.</p><p><img src=https://miro.medium.com/max/247/0*LN88EvcZzCZucYTw.png alt=img>
<img src=https://miro.medium.com/max/290/0*v51i5LqgXYGxfiTr.png alt=img>
<img src=https://miro.medium.com/max/296/0*XFb1QCF0VIk_gdKd.png alt=img></p><h4 id=iv-imputing-missing-values>iv. Imputing Missing Values</h4><p>It is hard to find a real-life dataset without having missing values generated due to miscellaneous reasons. You can neglect them by simply removing entire rows containing some missing features. But there is more to it. Exploring them can lead you to the issues spawning them from behind the scenes. Here’s Jeremy Howard, creator of the Fast.ai course and ex-president of Kaggle giving <a href="https://youtu.be/YSFG_W8JxBo?t=1h11m16s">an example</a>.</p><p>Instead of deleting entire rows and reducing the dataset size, we can fill them using their mean, median or mode. You can also use algorithms like k-NN and <a href=https://datascienceplus.com/imputing-missing-data-with-r-mice-package/>MICE</a> to do so. There’s no best way to fill the missing values. A technique that works on a dataset with certain missing values may not work on another.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>df, y, nas <span style=color:#f92672>=</span> proc_df(df_raw, <span style=color:#e6db74>&#39;SalePrice&#39;</span>)
</code></pre></div><p><a href=https://github.com/fastai/fastai/blob/775dcd03f49f4f6385f19b503f954468b83afbea/old/fastai/structured.py#L298-L392>proc_df:</a> This Fast.ai function fills <code>empty/Nan</code> values with the median of the feature and splits the dataframe into dependent y and independent df variables. It also adds 1 to the categorical codes so they start from 0 instead of -1 (pandas default). You can choose a random subset from <code>df_raw</code> and can also make features 1-hot encoded.</p><p>You should now have a clean matrix of integers as your df with no empty values. We can save the df and split it into a training and validation set.</p><blockquote><p><em>We want our validation set the same properties of the test set. Thus, keep the size of the validation set the same as the test set.</em></p></blockquote><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Pandas dataframes saved in Feather format</span>
df<span style=color:#f92672>.</span>to_feather(<span style=color:#e6db74>&#39;tmp/bulldozers-clean&#39;</span>)
df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_feather(<span style=color:#e6db74>&#39;tmp/bulldozers-clean&#39;</span>)

<span style=color:#75715e># Splitting df and y into train and validation set</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>split_vals</span>(a,n):
    <span style=color:#66d9ef>return</span> a[:n]<span style=color:#f92672>.</span>copy(), a[n:]<span style=color:#f92672>.</span>copy()

n_valid <span style=color:#f92672>=</span> <span style=color:#ae81ff>12000</span>  <span style=color:#75715e># Same size as test set</span>
n_trn <span style=color:#f92672>=</span> len(df) <span style=color:#f92672>-</span> n_valid

X_train, X_valid <span style=color:#f92672>=</span> split_vals(df, n_trn)
y_train, y_valid <span style=color:#f92672>=</span> split_vals(y, n_trn)
</code></pre></div><ul><li>Training set size: 389,125</li><li>Validation set size: 12,000</li></ul><p><img src=https://miro.medium.com/max/325/0*dbY-LZKoAoQNe2Jo.png alt=img></p><h3 id=random-forest-interpretation>Random Forest Interpretation</h3><p>A set of <a href=https://en.wikipedia.org/wiki/Decision_tree>decision trees</a> trained on a <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapped dataset</a> (random sampling with replacement of the same size as the original dataset (389,125)) is called a random forest. Scikit-learn provides a <code>RandomForestRegressor</code> for predicting continuous floating-point numbers and a <code>RandomForestClassifier</code> for discrete numbers.</p><p>Let us train a random forest regressor and take a look at its predictions.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> RandomForestRegressor, RandomForestClassifier

m <span style=color:#f92672>=</span> RandomForestRegressor(n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
m<span style=color:#f92672>.</span>fit(X_train, y_train)

m<span style=color:#f92672>.</span>predict(X_valid), m<span style=color:#f92672>.</span>score(X_valid, y_valid), np<span style=color:#f92672>.</span>mean(m<span style=color:#f92672>.</span>predict(X_valid)), np<span style=color:#f92672>.</span>std(m<span style=color:#f92672>.</span>predict(X_valid))

<span style=color:#f92672>-------</span>
Output:

<span style=color:#f92672>&gt;</span> (array([<span style=color:#ae81ff>9.17702</span>, <span style=color:#ae81ff>9.16897</span>, <span style=color:#ae81ff>9.25441</span>, <span style=color:#f92672>...</span>, <span style=color:#ae81ff>9.44273</span>, <span style=color:#ae81ff>9.31048</span>, <span style=color:#ae81ff>9.31048</span>]),
<span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.8913676814741088</span>,
<span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>10.012604788545604</span>,
<span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.6932527067952876</span>)
</code></pre></div><p><img src=https://miro.medium.com/max/282/0*WaUnLoDw6sK6v_Eq.png alt=img></p><h3 id=predictions-and-feature-contributions>Predictions and Feature Contributions</h3><p>The main output of a model is its predictions. They lie around 10.01 and have a standard deviation of 0.69. If we want to know how much each feature contributes to the prediction, we can do that using a <a href=https://github.com/andosa/treeinterpreter>Tree Interpreter</a>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> treeinterpreter <span style=color:#f92672>import</span> treeinterpreter <span style=color:#66d9ef>as</span> ti
prediction, bias, contributions <span style=color:#f92672>=</span> ti<span style=color:#f92672>.</span>predict(m, row)
</code></pre></div><p>The bias (10.10561) mentioned above is the mean of y in the training set. That is what we&rsquo;ll add to the contributions[-0.00458, -0.00525, &mldr; 0.00227, 0], to get the prediction (9.17702). Let us see them in a sorted manner.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>idxs <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argsort(contributions[<span style=color:#ae81ff>0</span>])
[o <span style=color:#66d9ef>for</span> o <span style=color:#f92672>in</span> zip(X_valid<span style=color:#f92672>.</span>columns[idxs], X_valid<span style=color:#f92672>.</span>iloc[<span style=color:#ae81ff>0</span>][idxs], contributions[<span style=color:#ae81ff>0</span>][idxs])]
<span style=color:#75715e># Feature,    Value in the row,    Contribution</span>

<span style=color:#f92672>----</span>
Output:

<span style=color:#f92672>&gt;</span> [(<span style=color:#e6db74>&#39;ProductSize&#39;</span>, <span style=color:#ae81ff>5</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.8274220325463135</span>),
<span style=color:#f92672>&gt;</span>  (<span style=color:#e6db74>&#39;saleElapsed&#39;</span>, <span style=color:#ae81ff>1284595200</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.06203234395696224</span>),
<span style=color:#f92672>&gt;</span>  (<span style=color:#e6db74>&#39;fiProductClassDesc&#39;</span>, <span style=color:#ae81ff>17</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.05853101269602874</span>),
<span style=color:#f92672>&gt;</span>  (<span style=color:#e6db74>&#39;fiModelDesc&#39;</span>, <span style=color:#ae81ff>3232</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.03869971271031911</span>),
<span style=color:#f92672>&gt;</span>  <span style=color:#f92672>...</span>
<span style=color:#f92672>&gt;</span>  (<span style=color:#e6db74>&#39;fiModelSeries&#39;</span>, <span style=color:#ae81ff>69</span>, <span style=color:#ae81ff>0.00821771617784126</span>),
<span style=color:#f92672>&gt;</span>  (<span style=color:#e6db74>&#39;fiBaseModel&#39;</span>, <span style=color:#ae81ff>1111</span>, <span style=color:#ae81ff>0.018305760128304628</span>),
<span style=color:#f92672>&gt;</span>  (<span style=color:#e6db74>&#39;YearMade&#39;</span>, <span style=color:#ae81ff>1999</span>, <span style=color:#ae81ff>0.024869657037691794</span>),
<span style=color:#f92672>&gt;</span>  (<span style=color:#e6db74>&#39;Coupler_System&#39;</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.10542811579794246</span>)]
</code></pre></div><h3 id=r-score>R² Score</h3><p>An equally important output of the model is the R² score.</p><blockquote><p><em>R² score: the ratio between how good your model is vs. how good a naïve mean model is (both measured using RMSE).</em></p></blockquote><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>r2_score</span>(actual, pred):
    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> rmse(actual, pred)<span style=color:#f92672>/</span> rmse(actual, np<span style=color:#f92672>.</span>mean(actual))
</code></pre></div><p>It is not the value we are optimizing for, but it lets you compare different models and get a sense of how a score of 0.8 compares to 0.9. A common misconception is that its value can only range from 0 to 1. But,</p><blockquote><p><em>If you predicted infinity for every row, R² = 1 −∞. So when your R² is negative, it means your model is worse than predicting the mean.</em></p></blockquote><p>And our model gives an R² score of 89% on the validation set with a 69% variance in the predictions, out of the box. We’ll treat this as our baseline.</p><h3 id=decision-tree-interpretation>Decision Tree Interpretation</h3><p>Let us zoom into a single tree:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, max_depth<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,
                          bootstrap<span style=color:#f92672>=</span>False, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
m<span style=color:#f92672>.</span>fit(X_train, y_train)draw_tree(m<span style=color:#f92672>.</span>estimators_[<span style=color:#ae81ff>0</span>], X_train, precision<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</code></pre></div><p><img src=https://miro.medium.com/max/573/0*XoBMr4-jU-Iy7Wn6.png alt=img></p><p>If <code>max_depth=3</code> were removed, the tree would continue to split until every node has the no. of samples=1. But now the tree is only 3 layers deep. At the root node of the tree, is the whole training set (samples=389125) and a value (avg y) of 10.106. Then it is split into 2 samples with size 348,393 and 40,732 at <code>Coupler_system &lt;= 0.5</code>.</p><h3 id=how-do-we-find-the-best-split>How Do We Find the Best Split?</h3><p>Let us consider that we are trying to split a node containing dogs and cats. If we split so that one node has all dogs and the other has all cats, we would see that the standard deviation of both is 0. Also the size of the child nodes matters.</p><p>Thus, we need to minimize this split score using brute force:</p><blockquote><p><em>Std_dev(y) in left child node * no. of points in the left child node</em></p><p>plus</p><p><em>Std_dev(y) in right child node * no. of points in the right child node</em></p></blockquote><p>Which is mathematically equivalent to:</p><blockquote><p><em>Mean squared error between each point in y and the avg y</em></p></blockquote><p>We can control the architecture of the forest using these parameters:</p><ul><li>Min samples leaf</li><li>Max features</li><li>No of estimators</li></ul><h4 id=i-min-samples-leaf>i. Min Samples Leaf</h4><p>You can control the negative depth of the tree using this argument. It stops dividing the nodes once there are 3 samples or less in the node. This gives a performance increase, as the depth of the tree has been reduced by one or two.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>,
                          min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</code></pre></div><p>It makes an individual tree’s predictions less accurate. As in each leaf node, it averages those 3 data points to give a prediction. But for the same reason, it generalizes better. The numbers that work well are 1, 3, 5, 10, 25, but it is relative to your overall dataset size.</p><h4 id=ii-max-features>ii. Max Features</h4><p>Similar to sampling rows while bootstrapping, we can sample features before choosing to split on it.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>,
                          min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)
</code></pre></div><p>That’s important because, if there exists a super important feature, every tree would make that their first split. Thus your trees will have a high correlation. Which we do not want. There might exist other important interactions between features that might not be explored.</p><p>The above forest samples <code>0.5*no_of_features</code>, and then brute forces through them to find the best split. Other good values are <code>sqrt</code> and <code>log2</code>.</p><h4 id=iii-no-of-estimators>iii. No of Estimators</h4><p>Boosting the no. of trees might increase the accuracy. But the graph flattens out. We can use this parameter as our last resort to stretch it out to its limits.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> sklearn <span style=color:#f92672>import</span> metrics
preds <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>stack([tree<span style=color:#f92672>.</span>predict(X_valid) <span style=color:#66d9ef>for</span> tree <span style=color:#f92672>in</span> m<span style=color:#f92672>.</span>estimators_])
plt<span style=color:#f92672>.</span>plot([metrics<span style=color:#f92672>.</span>r2_score(y_valid, np<span style=color:#f92672>.</span>mean(preds[:i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>], axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>40</span>)]);
</code></pre></div><p><img src=https://miro.medium.com/max/247/0*K52fjVdWcfT1lNz4.png alt=img></p><h3 id=bagging-and-subsampling>Bagging and Subsampling</h3><p>As mentioned earlier, random forests by default are built on a <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapped</a> sample. The sample is picked randomly with replacement but is the same size as the entire training set for each tree (389,125). And the collective predictions from the trees is averaged in the end. This is what <a href=https://en.wikipedia.org/wiki/Leo_Breiman>Leo Breiman</a>, the creator of random forests describes as bootstrap aggregation (bagging for short).</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
<span style=color:#f92672>%</span>time m<span style=color:#f92672>.</span>fit(X_train, y_train)
print_score(m)

<span style=color:#f92672>----</span>
Output:

<span style=color:#f92672>&gt;</span> CPU times: user <span style=color:#ae81ff>6</span>min <span style=color:#ae81ff>50</span>s, sys: <span style=color:#ae81ff>4.37</span> s, total: <span style=color:#ae81ff>6</span>min <span style=color:#ae81ff>54</span>s
<span style=color:#f92672>&gt;</span> Wall time: <span style=color:#ae81ff>2</span>min <span style=color:#ae81ff>15</span>s
<span style=color:#f92672>&gt;</span> RMSE Train:  <span style=color:#ae81ff>7.856880831112418</span>
<span style=color:#f92672>&gt;</span> RMSE Valid:  <span style=color:#ae81ff>23.892947209645595</span>
<span style=color:#f92672>&gt;</span>  R2  Train:  <span style=color:#ae81ff>98.70986499747674</span>
<span style=color:#f92672>&gt;</span>  R2  Valid:  <span style=color:#ae81ff>89.80499551721383</span>
</code></pre></div><p>Training on so much duplicate data is unnecessary for experimentation. It raises the training time to more than 2 mins and is overfitting on our training set. And unlike 1999, when Breiman introduced the world to bagging, we have a lot of data to train our models on.</p><blockquote><p><em>So instead of sampling rows with replacement for each tree during bagging, why not sample them without replacement?</em></p></blockquote><p><img src=https://miro.medium.com/max/247/0*IiHNFeVCYVTn0WFo.jpg alt=img></p><p>This is called subsampling. It is like having a team of scientists, working on different datasets of a common problem. Because their datasets have nothing in common, the system generalizes better. And given enough scientists, the team will surely go through all the data.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>set_rf_samples(<span style=color:#ae81ff>20000</span>)
m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
<span style=color:#f92672>%</span>time m<span style=color:#f92672>.</span>fit(X_train, y_train)
print_score(m)

<span style=color:#f92672>----</span>
Output:

<span style=color:#f92672>&gt;</span> CPU times: user <span style=color:#ae81ff>40.6</span> s, sys: <span style=color:#ae81ff>890</span> ms, total: <span style=color:#ae81ff>41.5</span> s
<span style=color:#f92672>&gt;</span> Wall time: <span style=color:#ae81ff>19.3</span> s
<span style=color:#f92672>&gt;</span> RMSE Train:  <span style=color:#ae81ff>22.767005816704206</span>
<span style=color:#f92672>&gt;</span> RMSE Valid:  <span style=color:#ae81ff>26.17177986481096</span>
<span style=color:#f92672>&gt;</span>  R2  Train:  <span style=color:#ae81ff>89.16705188450162</span>
<span style=color:#f92672>&gt;</span>  R2  Valid:  <span style=color:#ae81ff>87.7675206461405</span>
</code></pre></div><p>This function, <a href=https://github.com/fastai/fastai/blob/775dcd03f49f4f6385f19b503f954468b83afbea/old/fastai/structured.py#L398-L409>set_rf_samples(20000)</a> gives each tree a different random sample of 20,000 rows to train on. It increases the variance in our predictions and trains faster as each tree gets a smaller dataset.</p><p>This function is not available in scikit-learn, and is a workaround made by Jeremy Howard as a part of the fast.ai library. You can reset to a normal bootstrapped model by using <code>reset_rf_samples()</code>.</p><h3 id=when-to-use-what>When to Use What?</h3><p>According to Breiman, there are 2 things you are trying to balance while using random forests:</p><blockquote><p><em>Each tree (aka estimators) is trained as well as possible.</em>
<em>But across estimators, the correlation between them is as low as possible.</em></p></blockquote><p>Subsampling gives speed and an ability to generalize. Thus it can be used for any kind of feature engineering or experimentation. Bagging helps train the trees very well. We’ll use this after we freeze the architecture and features.</p><h3 id=bonus-metric--oob-score>Bonus Metric — OOB Score:</h3><p>Whether we sample with (bootstrapping) or without replacement (subsampling), not all the rows in the training set will be used for each tree. This gives random forests a free validation set. Calculating the R² using these out of the bag (OOB) rows as a validation set gives us an idea of how well is the model generalizing over the training set. It is mostly used with bagging.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>reset_rf_samples()
m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,
                      max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, oob_score<span style=color:#f92672>=</span>True)
<span style=color:#f92672>%</span>time m<span style=color:#f92672>.</span>fit(X_train, y_train)
print_score(m)

<span style=color:#f92672>----</span>
Output:

<span style=color:#f92672>&gt;</span> CPU times: user <span style=color:#ae81ff>3</span>min <span style=color:#ae81ff>13</span>s, sys: <span style=color:#ae81ff>2.6</span> s, total: <span style=color:#ae81ff>3</span>min <span style=color:#ae81ff>16</span>s
<span style=color:#f92672>&gt;</span> Wall time: <span style=color:#ae81ff>1</span>min <span style=color:#ae81ff>6</span>s
<span style=color:#f92672>&gt;</span> RMSE Train:  <span style=color:#ae81ff>11.906626005969184</span>
<span style=color:#f92672>&gt;</span> RMSE Valid:  <span style=color:#ae81ff>22.80435877833463</span>
<span style=color:#f92672>&gt;</span>  R2  Train:  <span style=color:#ae81ff>97.03713235875033</span>
<span style=color:#f92672>&gt;</span>  R2  Valid:  <span style=color:#ae81ff>90.71282339508942</span>
<span style=color:#f92672>&gt;</span>    OOB    :  <span style=color:#ae81ff>91.19405377806355</span>
</code></pre></div><p>Be careful while using the OOB score with <code>set_rf_samples(20000)</code>. If the training set consists of a million rows and you give each tree around 20k rows, the validation set for an OOB score would be huge.</p><p><img src=https://miro.medium.com/max/304/0*Cj4l8QQ0nYREk3bC.jpg alt=img></p><p>That’s enough detail about the random forests for now. In the <a href=https://www.sdhnshu.com/showcase/random-forests-2>part 2</a> we’ll see how this algorithm helps us with feature engineering.</p><hr><p><strong><em>Continue reading <a href=https://www.sdhnshu.com/showcase/random-forests-2>Part 2</a></em></strong></p><h3 id=further-reading>Further Reading</h3><ul><li><a href=https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-1-84a1dc2b5236>Fast.ai ML course notes</a></li><li><a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4120293/>Log-transformation and its implications for data analysis</a></li><li><a href=https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779>6 Different Ways to Compensate for Missing Values In a Dataset</a></li><li><a href=https://machinelearningmastery.com/how-to-calculate-nonparametric-rank-correlation-in-python/>How to Calculate Nonparametric Rank Correlation in Python</a></li><li><a href=https://machinelearningmastery.com/data-leakage-machine-learning/>Data leakage in machine learning</a></li></ul></div></div></div></div></div></div></div><script src=https://www.sdhnshu.com/js/jquery.min.js></script><script src=https://www.sdhnshu.com/js/bootstrap.min.js></script><script src=https://www.sdhnshu.com/js/jquery.cookie.js></script><script src=https://www.sdhnshu.com/js/ekko-lightbox.js></script><script src=https://www.sdhnshu.com/js/jquery.scrollTo.min.js></script><script src=https://www.sdhnshu.com/js/masonry.pkgd.min.js></script><script src=https://www.sdhnshu.com/js/imagesloaded.pkgd.min.js></script><script src=https://www.sdhnshu.com/js/owl.carousel.min.js></script><script src=https://www.sdhnshu.com/js/front.js></script></body></html>