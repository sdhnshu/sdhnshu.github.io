<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Sudhanshu Passi - Feature Engineering with Random Forests (Part 2)</title><meta name=description content="Sudhanshu Passi's personal website"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><link rel=stylesheet href=https://www.sdhnshu.com/css/bootstrap.min.css><link href="https://fonts.googleapis.com/css2?family=Lato&family=Merriweather:wght@300&display=swap" rel=stylesheet><link rel=stylesheet href=https://www.sdhnshu.com/css/font-awesome.min.css><link rel=stylesheet href=https://www.sdhnshu.com/css/owl.carousel.css><link rel=stylesheet href=https://www.sdhnshu.com/css/owl.theme.css><link href=https://www.sdhnshu.com/css/style.violet.css rel=stylesheet id=theme-stylesheet><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/atom-one-dark.min.css><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><link href=https://www.sdhnshu.com/css/custom.css rel=stylesheet><link rel="shortcut icon" href=https://www.sdhnshu.com/img/favicon.ico><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-173272544-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div id=all><div class=container-fluid><div class="row row-offcanvas row-offcanvas-left"><div id=sidebar class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas"><div class=sidebar-content><h1 class=sidebar-heading><a href=https://www.sdhnshu.com/>Sudhanshu Passi</a></h1><ul class=sidebar-menu><li><a href=https://www.sdhnshu.com/showcase/>Showcase</a></li><li><a href=https://www.sdhnshu.com/experiments/>Experiments</a></li><li><a href=https://www.sdhnshu.com/about/>About</a></li></ul><p class=social><a href=https://github.com/sdhnshu data-animate-hover=pulse class=external><i class="fa fa-github"></i></a><a href=mailto:sudhanshupassi@gmail.com data-animate-hover=pulse class=email><i class="fa fa-envelope"></i></a><a href=https://www.linkedin.com/in/sdhnshu/ data-animate-hover=pulse class=external><i class="fa fa-linkedin"></i></a><a href=https://twitter.com/Sudhanshupassi data-animate-hover=pulse class="external twitter"><i class="fa fa-twitter"></i></a><a href=https://medium.com/@sdhnshu data-animate-hover=pulse class=external><i class="fa fa-medium"></i></a></p><div class=copyright><p class=credit>&copy; 2020 Sudhanshu Passi</p></div></div></div><div class="col-xs-12 col-sm-8 col-md-9 content-column white-background"><div class="small-navbar visible-xs"><button type=button data-toggle=offcanvas class="btn btn-ghost pull-left"> <i class="fa fa-align-left"></i>Menu</button><h1 class=small-navbar-heading><a href=https://www.sdhnshu.com/>Sudhanshu Passi</a></h1></div><div class=row><div class=col-lg-11><div class=content-column-content><h1>Feature Engineering with Random Forests (Part 2)</h1><i><p class=timestamp>Last updated Apr 29, 2020</p></i><p>Learn how to leverage Random Forests to do Feature Engineering and more.</p><ul><li>Originally published on <a href=https://medium.com/@sdhnshu/feature-engineering-with-random-forests-part2-160eb0356172>Medium</a></li><li>Grab the code from <a href=https://github.com/sdhnshu/Random-forests-and-Feature-Engineering>Github</a></li><li>Link to <a href=https://www.sdhnshu.com/showcase/random-forests>Part 1</a></li></ul><h3 id=introduction>Introduction</h3><p>In the <a href=https://www.sdhnshu.com/showcase/random-forests>part 1</a> of the series, we looked at the random forests algorithm and figured how to get accurate predictions from the model. But the part we are forgetting is:</p><blockquote><p><em>The features that get into the model are as important as the model itself.</em></p></blockquote><p>Especially if you work at an institution, like the one making <a href=https://www.kaggle.com/c/bluebook-for-bulldozers/data>bulldozers</a>, you are trying to understand the following things about the features:</p><ul><li>What is the <strong>importance</strong> of each feature?</li><li>How <strong>similar</strong> are they to each other?</li><li>How do these features <strong>interact</strong> with each other?</li><li>How to make features truly independent from <strong>time</strong>?</li></ul><p>Let us look at each of them using concepts from random forests.</p><h3 id=i-feature-importance>i. Feature Importance</h3><p>The only way to find the feature importance is rightly described by one of my oldest inspirations in this verse:</p><blockquote><p><em>You don’t know what you’ve got, until its gone — <a href="https://www.youtube.com/watch?v=oM-XJD4J36U">Chester Bennington</a></em></p></blockquote><p>Technically speaking, if we randomly shuffle the values in a feature but keep everything else the same, how much worse does our model perform. The worse the model performs, the higher the importance of that feature.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># Feature Importance Intuition</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>feature_importance</span>(self, model, x_valid):

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>shuffle_col</span>(colname, x_valid):
        df_temp <span style=color:#f92672>=</span> x_valid<span style=color:#f92672>.</span>copy()
        df_temp[colname] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>permutation(
            df_temp[colname]<span style=color:#f92672>.</span>values)
        <span style=color:#66d9ef>return</span> df_temp

    pred_actual <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(x_valid)
    <span style=color:#66d9ef>return</span> [(<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> r2_score(pred_actual, model<span style=color:#f92672>.</span>predict(
        shuffle_col(col, x_valid)))) <span style=color:#66d9ef>for</span> col <span style=color:#f92672>in</span> x_valid<span style=color:#f92672>.</span>columns]
</code></pre></div><p>The above function describes the functionality of feature importance and the one below is a convenient Fast.ai wrapper around the scikit learn’s optimized functionality. Let us plot 30 most important features out of the 66 we have.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>set_rf_samples(<span style=color:#ae81ff>50000</span>)
m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,
                          max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>,
                          oob_score<span style=color:#f92672>=</span>True)
m<span style=color:#f92672>.</span>fit(X_train, y_train)
<span style=color:#75715e># rf_feat_importance wrapper from Fast.ai</span>
fi <span style=color:#f92672>=</span> rf_feat_importance(m, df_trn)
fi[:<span style=color:#ae81ff>30</span>]<span style=color:#f92672>.</span>plot(<span style=color:#e6db74>&#39;cols&#39;</span>, <span style=color:#e6db74>&#39;imp&#39;</span>, <span style=color:#e6db74>&#39;barh&#39;</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>12</span>,<span style=color:#ae81ff>7</span>), legend<span style=color:#f92672>=</span>False)
</code></pre></div><p><img src=https://miro.medium.com/max/573/0*BcS_MXeq3Kv-ibQf.png alt=img></p><p>We can remove the ones lower than the importance of 0.005. This leaves us with 23 features. Let us plot the importance again and score the model.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>to_keep <span style=color:#f92672>=</span> fi[fi<span style=color:#f92672>.</span>imp<span style=color:#f92672>&gt;</span><span style=color:#ae81ff>0.005</span>]<span style=color:#f92672>.</span>cols
df_keep <span style=color:#f92672>=</span> df_trn[to_keep]<span style=color:#f92672>.</span>copy()
X_train, X_valid <span style=color:#f92672>=</span> split_vals(df_keep, n_trn)
m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,
                       max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, oob_score<span style=color:#f92672>=</span>True)
m<span style=color:#f92672>.</span>fit(X_train, y_train)
print_score(m)
fi <span style=color:#f92672>=</span> rf_feat_importance(m, df_keep)
plot_fi(fi[:<span style=color:#ae81ff>30</span>]);

<span style=color:#f92672>----</span>
Output:
<span style=color:#f92672>&gt;</span> RMSE Train:  <span style=color:#ae81ff>20.720376603129303</span>
<span style=color:#f92672>&gt;</span> RMSE Valid:  <span style=color:#ae81ff>24.570217524283752</span>
<span style=color:#f92672>&gt;</span>  R2  Train:  <span style=color:#ae81ff>91.02715603822325</span>
<span style=color:#f92672>&gt;</span>  R2  Valid:  <span style=color:#ae81ff>89.21882794959521</span>
<span style=color:#f92672>&gt;</span>    OOB    :  <span style=color:#ae81ff>89.39924413165484</span>
</code></pre></div><p><img src=https://miro.medium.com/max/573/0*soe_Udg2GtaRkXWC.png alt=img></p><p><code>YearMade</code>, <code>Coupler_System</code> and <code>ProductSize</code> turn out to be the top 3 most important features and we have a validation score of 89.2%</p><h3 id=ii-feature-correlation>ii. Feature correlation</h3><p>Combining similar features can be done by going through all the pairs and clustering the ones that are closest to each other. This kind of hierarchical clustering works based on the ordinality of those features. Which is similar to how random forests work. While splitting a node, it only cares about the sorted order of the data and not their actual values. A common coefficient to measure for this kind of ranked correlation is spearman’s ranked coefficient.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> scipy.cluster <span style=color:#f92672>import</span> hierarchy <span style=color:#66d9ef>as</span> hc
corr <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>round(scipy<span style=color:#f92672>.</span>stats<span style=color:#f92672>.</span>spearmanr(df_keep)<span style=color:#f92672>.</span>correlation, <span style=color:#ae81ff>4</span>)
plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>16</span>,<span style=color:#ae81ff>10</span>))
hc<span style=color:#f92672>.</span>dendrogram(hc<span style=color:#f92672>.</span>linkage(hc<span style=color:#f92672>.</span>distance<span style=color:#f92672>.</span>squareform(<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>corr),
                         method<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;average&#39;</span>),
              labels<span style=color:#f92672>=</span>df_keep<span style=color:#f92672>.</span>columns, orientation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;left&#39;</span>,
              leaf_font_size<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>)
plt<span style=color:#f92672>.</span>show()
</code></pre></div><p><img src=https://miro.medium.com/max/573/0*8zHuirGqrQL55zdU.png alt=img></p><p>The features that are correlated connect faster. With that, we can see that <code>saleYear</code> and <code>saleElapsed</code> are similar which is expected as they both are time-dependent. <code>Hydraulics_flow</code> and <code>Grouser_Tracks</code> are similar and also <code>fiBaseModel</code> and <code>fiModelDesc</code>.</p><p>If two features are similar removing one of them won’t affect the accuracy of our model, it will only make it simpler. So let us try dropping them and measuring the drop in score. Before that let’s establish a baseline.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_oob</span>(df):
    m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>30</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>,
                              max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>,
                              oob_score<span style=color:#f92672>=</span>True)
    x, _ <span style=color:#f92672>=</span> split_vals(df, n_trn)
    m<span style=color:#f92672>.</span>fit(x, y_train)
    <span style=color:#66d9ef>return</span> m<span style=color:#f92672>.</span>oob_score_ <span style=color:#f92672>*</span> <span style=color:#ae81ff>100</span>

get_oob(df_keep)

<span style=color:#f92672>---</span>
Output:

<span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>89.03929035253717</span>   <span style=color:#75715e># Baseline</span>

</code></pre></div><p>Let’s drop them one by one.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#39;saleYear&#39;</span>, <span style=color:#e6db74>&#39;saleElapsed&#39;</span>, <span style=color:#e6db74>&#39;Grouser_Tracks&#39;</span>, <span style=color:#e6db74>&#39;Hydraulics_Flow&#39;</span>, <span style=color:#e6db74>&#39;fiModelDesc&#39;</span>, <span style=color:#e6db74>&#39;fiBaseModel&#39;</span>]:
    <span style=color:#66d9ef>print</span>(c, get_oob(df_keep<span style=color:#f92672>.</span>drop(c, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)))

<span style=color:#f92672>-----</span>
Output:

<span style=color:#f92672>&gt;</span> saleYear <span style=color:#ae81ff>88.96577294296006</span>
<span style=color:#f92672>&gt;</span> saleElapsed <span style=color:#ae81ff>88.72699565865227</span>
<span style=color:#f92672>&gt;</span> Grouser_Tracks <span style=color:#ae81ff>89.02477486920333</span>
<span style=color:#f92672>&gt;</span> Hydraulics_Flow <span style=color:#ae81ff>88.98468803187907</span>
<span style=color:#f92672>&gt;</span> fiModelDesc <span style=color:#ae81ff>88.93420811313398</span>
<span style=color:#f92672>&gt;</span> fiBaseModel <span style=color:#ae81ff>88.91058697856653</span>
</code></pre></div><p>Because they didn’t do much damage, <code>saleYear</code>, <code>Hydraulics_Flow</code>, and <code>fiBaseModel</code> shall be removed.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>to_drop <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;saleYear&#39;</span>, <span style=color:#e6db74>&#39;Hydraulics_Flow&#39;</span>, <span style=color:#e6db74>&#39;fiBaseModel&#39;</span>]
get_oob(df_keep<span style=color:#f92672>.</span>drop(to_drop, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))

<span style=color:#f92672>---</span>
Output:

<span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>88.89865421652972</span>
</code></pre></div><p>The overall OOB score dropped a little but not significantly. Let us check how a full bootstrapped model is looking with the cleaned data. It happens in 30 seconds rather than 1 min in the last post!</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>reset_rf_samples()

df_keep<span style=color:#f92672>.</span>drop(to_drop, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, inplace<span style=color:#f92672>=</span>True)
X_train, X_valid <span style=color:#f92672>=</span> split_vals(df_keep, n_trn)

m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, oob_score<span style=color:#f92672>=</span>True)
<span style=color:#f92672>%</span>time m<span style=color:#f92672>.</span>fit(X_train, y_train)

print_score(m)

<span style=color:#f92672>-----</span>
Output:

<span style=color:#f92672>&gt;</span> CPU times: user <span style=color:#ae81ff>1</span>min <span style=color:#ae81ff>45</span>s, sys: <span style=color:#ae81ff>922</span> ms, total: <span style=color:#ae81ff>1</span>min <span style=color:#ae81ff>46</span>s
<span style=color:#f92672>&gt;</span> Wall time: <span style=color:#ae81ff>30.4</span> s
<span style=color:#f92672>&gt;</span> RMSE Train:  <span style=color:#ae81ff>12.51548564415119</span>
<span style=color:#f92672>&gt;</span> RMSE Valid:  <span style=color:#ae81ff>22.79049206209574</span>
<span style=color:#f92672>&gt;</span>  R2  Train:  <span style=color:#ae81ff>96.72636512376931</span>
<span style=color:#f92672>&gt;</span>  R2  Valid:  <span style=color:#ae81ff>90.72411452493563</span>
<span style=color:#f92672>&gt;</span>    OOB    :  <span style=color:#ae81ff>90.86147528243829</span>
</code></pre></div><h3 id=iii-partial-dependence>iii. Partial dependence</h3><p>In this section we’ll see how two features interact. But before we get into it, it would be interesting to see the interactions between specific categories from a categorical variable as well. We can one-hot encode categorical variables with less than 7 categories to do that. Let us check their feature importance.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>set_rf_samples(<span style=color:#ae81ff>50000</span>) <span style=color:#75715e># Subsampled model</span>

df_trn2, y_trn, nas <span style=color:#f92672>=</span> proc_df(df_raw, <span style=color:#e6db74>&#39;SalePrice&#39;</span>, max_n_cat<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>)  <span style=color:#75715e># 1hot encoded</span>
X_train, X_valid <span style=color:#f92672>=</span> split_vals(df_trn2, n_trn)

m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
m<span style=color:#f92672>.</span>fit(X_train, y_train);

fi <span style=color:#f92672>=</span> rf_feat_importance(m, df_trn2)
plot_fi(fi[:<span style=color:#ae81ff>20</span>]);
</code></pre></div><p><img src=https://miro.medium.com/max/573/0*WYgcNZUdWFGQeDOW.png alt=img></p><p>Enclosure was a categorical feature but is now one-hot encoded. And <code>Enclosure_EROPS w AC</code> has gained the highest importance among all. Some search on the internet reveals that it stands for an enclosed space with an AC. Which, if in a bulldozer will surely be a deciding factor for its price.</p><p>The second most important feature is <code>YearMade</code>. It would be logical to see its interacts with 7th most important, <code>saleElapsed</code>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>df_raw<span style=color:#f92672>.</span>plot(<span style=color:#e6db74>&#39;YearMade&#39;</span>, <span style=color:#e6db74>&#39;saleElapsed&#39;</span>, <span style=color:#e6db74>&#39;scatter&#39;</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>8</span>));
</code></pre></div><p><img src=https://miro.medium.com/max/450/0*gHFbPT5HE9JLgWRl.png alt=img></p><p>Oh no! There are some outliers in the data with <code>YearMade=1000</code>. This might be due to some technical error. We can remove them from our analyses for now.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>df_raw <span style=color:#f92672>=</span> df_raw[df_raw<span style=color:#f92672>.</span>YearMade<span style=color:#f92672>&gt;</span><span style=color:#ae81ff>1930</span>]
</code></pre></div><p>Let us see how <code>YearMade</code> affects our dependent variable, <code>SalePrice</code> by simply plotting them together.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> plotnine <span style=color:#f92672>import</span> <span style=color:#f92672>*</span>

x_all <span style=color:#f92672>=</span> get_sample(df_raw[df_raw<span style=color:#f92672>.</span>YearMade<span style=color:#f92672>&gt;</span><span style=color:#ae81ff>1930</span>], <span style=color:#ae81ff>500</span>)  <span style=color:#75715e># sample 500 pts excluding years with wrong year=1000</span>
ggplot(x_all, aes(<span style=color:#e6db74>&#39;YearMade&#39;</span>, <span style=color:#e6db74>&#39;SalePrice&#39;</span>))<span style=color:#f92672>+</span>stat_smooth(se<span style=color:#f92672>=</span>True, method<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;loess&#39;</span>)
</code></pre></div><p><img src=https://miro.medium.com/max/408/0*eRH40X0btXLy10ae.png alt=img></p><p>This plot shows ( <code>YearMade</code> + the rest of the features)&rsquo;s affect on <code>SalePrice</code>.</p><p>To single out how a feature affects another, we need to change one and keep the rest constant. In the example below, we sampled 500 rows and changed all values of <code>YearMade</code> in the dataset to a value on x, eg: 1990 and predicted a value of <code>SalePrice</code>. Doing so for all values of <code>YearMade</code> gives us the plot.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>x <span style=color:#f92672>=</span> get_sample(X_train[X_train<span style=color:#f92672>.</span>YearMade<span style=color:#f92672>&gt;</span><span style=color:#ae81ff>1930</span>], <span style=color:#ae81ff>500</span>)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_pdp</span>(feat, clusters<span style=color:#f92672>=</span>None, feat_name<span style=color:#f92672>=</span>None):
    feat_name <span style=color:#f92672>=</span> feat_name <span style=color:#f92672>or</span> feat
    p <span style=color:#f92672>=</span> pdp<span style=color:#f92672>.</span>pdp_isolate(m, x, feat)
    <span style=color:#66d9ef>return</span> pdp<span style=color:#f92672>.</span>pdp_plot(p, feat_name, plot_lines<span style=color:#f92672>=</span>True,
                        cluster<span style=color:#f92672>=</span>clusters <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> None,
                        n_cluster_centers<span style=color:#f92672>=</span>clusters)
plot_pdp(<span style=color:#e6db74>&#39;YearMade&#39;</span>)
plot_pdp(<span style=color:#e6db74>&#39;YearMade&#39;</span>, <span style=color:#ae81ff>5</span>)
</code></pre></div><p><img src=https://miro.medium.com/max/573/0*vu4fB_9e_nDarbv7.png alt=img>
<img src=https://miro.medium.com/max/573/0*QGbSRpn3XQMHy_Wq.png alt=img></p><p>Similarly to find how <code>YearMade</code> and <code>SaleElapsed</code> together affect <code>SalePrice</code>, we can use an interaction plot.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>feats <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;saleElapsed&#39;</span>, <span style=color:#e6db74>&#39;YearMade&#39;</span>]
p <span style=color:#f92672>=</span> pdp<span style=color:#f92672>.</span>pdp_interact(m, x, feats)
pdp<span style=color:#f92672>.</span>pdp_interact_plot(p, feats)
</code></pre></div><p><img src=https://miro.medium.com/max/573/0*Wzx31D2CkGFSkYcO.png alt=img></p><p>We can see from the interaction plot that if <code>YearMade</code> is high and <code>saleElapsed</code> low, the <code>SalePrice</code> would be higher. So the <code>age</code> = <code>saleYear</code> - <code>YearMade</code> of the truck is important. You can add <code>age</code> as another feature to our dataset.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>df_raw<span style=color:#f92672>.</span>YearMade[df_raw<span style=color:#f92672>.</span>YearMade<span style=color:#f92672>&lt;</span><span style=color:#ae81ff>1950</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1950</span>
df_keep[<span style=color:#e6db74>&#39;age&#39;</span>] <span style=color:#f92672>=</span> df_raw[<span style=color:#e6db74>&#39;age&#39;</span>] <span style=color:#f92672>=</span> df_raw<span style=color:#f92672>.</span>saleYear<span style=color:#f92672>-</span>df_raw<span style=color:#f92672>.</span>YearMade
X_train, X_valid <span style=color:#f92672>=</span> split_vals(df_keep, n_trn)
m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,
                          max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
m<span style=color:#f92672>.</span>fit(X_train, y_train)
plot_fi(rf_feat_importance(m, df_keep));
</code></pre></div><p><img src=https://miro.medium.com/max/573/0*DjzkQSca-kwhl0UO.png alt=img></p><p><code>Age</code> quickly becomes our most important feature.</p><h3 id=iv-removing-time-dependency>iv. Removing time dependency</h3><p>Unlike linear models or neural networks, random forests are not very good at extrapolating data. So extrapolating over time is tough. It is better, we remove the dependency on time from our features.</p><p><strong>Q: But how to we find the time dependent features?</strong></p><blockquote><p><em>If the validation set were a random sample of the training set, it would be difficult to predict if a row is in the validation set.</em></p></blockquote><p>So if a model can successfully learn whether a value is in the validation set, then it has some temporal dependency helping it to do so. And the most important features in such a model will be the most time dependent features.</p><p>All we have to do is, make <code>is_valid</code> our dependent variable, and train the model. Because it is a 0/1 discrete prediction we are using a <code>RandomForestClassifier</code> instead of a regressor.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>df_ext <span style=color:#f92672>=</span> df_keep<span style=color:#f92672>.</span>copy()
df_ext[<span style=color:#e6db74>&#39;is_valid&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
df_ext<span style=color:#f92672>.</span>is_valid[:n_trn] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
x, y, nas <span style=color:#f92672>=</span> proc_df(df_ext, <span style=color:#e6db74>&#39;is_valid&#39;</span>)
m <span style=color:#f92672>=</span> RandomForestClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, oob_score<span style=color:#f92672>=</span>True)
m<span style=color:#f92672>.</span>fit(x, y);
m<span style=color:#f92672>.</span>oob_score_

<span style=color:#f92672>----</span>
Output:

<span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.9999950140230601</span>
</code></pre></div><p>There is a very high score of prediction, thus we have features with time dependency. The validation set is not a random sample from the dataset. Let us see which features are the most important to a purely temporal dependent forest.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>fi <span style=color:#f92672>=</span> rf_feat_importance(m, x); fi[:<span style=color:#ae81ff>10</span>]
</code></pre></div><p><img src=https://miro.medium.com/max/187/0*AAeuknC1bCxFAQRq.png alt=img></p><p>Our top 3 time dependent features are: <code>SalesId</code>, <code>saleElapsed</code> and <code>MachineId</code>. Let us remove them and see their effect on the score.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>x<span style=color:#f92672>.</span>drop([<span style=color:#e6db74>&#39;SalesID&#39;</span>, <span style=color:#e6db74>&#39;saleElapsed&#39;</span>, <span style=color:#e6db74>&#39;MachineID&#39;</span>], axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, inplace<span style=color:#f92672>=</span>True)
m <span style=color:#f92672>=</span> RandomForestClassifier(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, oob_score<span style=color:#f92672>=</span>True)
m<span style=color:#f92672>.</span>fit(x, y);
m<span style=color:#f92672>.</span>oob_score_

<span style=color:#f92672>---</span>
Output:

<span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.9788444998441882</span>
</code></pre></div><p>Checking the importance after removing those features</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>fi <span style=color:#f92672>=</span> rf_feat_importance(m, x); fi[:<span style=color:#ae81ff>10</span>]
</code></pre></div><p><img src=https://miro.medium.com/max/204/0*onbQPrnDELkYqKS0.png alt=img></p><p>Now we see new time-dependent features pop up like <code>age</code>, <code>YearMade</code>, and <code>saleDayofyear</code>. Let us drop these 6 one by one, and check their negative affects. Before that let us look at our baseline.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>set_rf_samples(<span style=color:#ae81ff>50000</span>)
X_train, X_valid <span style=color:#f92672>=</span> split_vals(df_keep, n_trn)
m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,
                        max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, oob_score<span style=color:#f92672>=</span>True)
m<span style=color:#f92672>.</span>fit(X_train, y_train)
print_score(m)

<span style=color:#f92672>----</span>
Output:

<span style=color:#f92672>&gt;</span> RMSE Train:  <span style=color:#ae81ff>20.633388115029838</span>
<span style=color:#f92672>&gt;</span> RMSE Valid:  <span style=color:#ae81ff>24.491469116919422</span>
<span style=color:#f92672>&gt;</span>  R2  Train:  <span style=color:#ae81ff>91.1023376550528</span>
<span style=color:#f92672>&gt;</span>  R2  Valid:  <span style=color:#ae81ff>89.2878252704439</span>
<span style=color:#f92672>&gt;</span>    OOB    :  <span style=color:#ae81ff>89.46118267044744</span>
</code></pre></div><p>Now let us drop them one by one.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>feats <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;SalesID&#39;</span>, <span style=color:#e6db74>&#39;saleElapsed&#39;</span>, <span style=color:#e6db74>&#39;MachineID&#39;</span>, <span style=color:#e6db74>&#39;age&#39;</span>, <span style=color:#e6db74>&#39;YearMade&#39;</span>, <span style=color:#e6db74>&#39;saleDayofyear&#39;</span>]
<span style=color:#66d9ef>for</span> f <span style=color:#f92672>in</span> feats:
    df_subs <span style=color:#f92672>=</span> df_keep<span style=color:#f92672>.</span>drop(f, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
    X_train, X_valid <span style=color:#f92672>=</span> split_vals(df_subs, n_trn)
    m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,
            max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, oob_score<span style=color:#f92672>=</span>True)
    m<span style=color:#f92672>.</span>fit(X_train, y_train)
    <span style=color:#66d9ef>print</span>(f)
    print_score(m)
</code></pre></div><p><img src=https://miro.medium.com/max/191/0*j5LmuOYm8Y5RPYFe.png alt=img></p><p>The R² score on the validation set gets better by dropping <code>SalesId</code>, <code>MachineId</code>, and <code>saleDayofyear</code>.</p><p>With that, our features are as time-independent as could be. So, let us drop them and train a complete bootstrap model.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>df_subs <span style=color:#f92672>=</span> df_keep<span style=color:#f92672>.</span>drop([<span style=color:#e6db74>&#39;SalesID&#39;</span>, <span style=color:#e6db74>&#39;MachineID&#39;</span>, <span style=color:#e6db74>&#39;saleDayofyear&#39;</span>], axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
reset_rf_samples()

X_train, X_valid <span style=color:#f92672>=</span> split_vals(df_subs, n_trn)
m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,
               max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, oob_score<span style=color:#f92672>=</span>True)

<span style=color:#f92672>%</span>time m<span style=color:#f92672>.</span>fit(X_train, y_train)
print_score(m)

plot_fi(rf_feat_importance(m, X_train));

<span style=color:#f92672>----</span>
Output:

<span style=color:#f92672>&gt;</span> CPU times: user <span style=color:#ae81ff>1</span>min <span style=color:#ae81ff>15</span>s, sys: <span style=color:#ae81ff>991</span> ms, total: <span style=color:#ae81ff>1</span>min <span style=color:#ae81ff>16</span>s
<span style=color:#f92672>&gt;</span> Wall time: <span style=color:#ae81ff>23.4</span> s
<span style=color:#f92672>&gt;</span> RMSE Train:  <span style=color:#ae81ff>13.775059373372189</span>
<span style=color:#f92672>&gt;</span> RMSE Valid:  <span style=color:#ae81ff>21.81697224146163</span>
<span style=color:#f92672>&gt;</span>  R2  Train:  <span style=color:#ae81ff>96.03428239799979</span>
<span style=color:#f92672>&gt;</span>  R2  Valid:  <span style=color:#ae81ff>91.49964757192211</span>
<span style=color:#f92672>&gt;</span>    OOB    :  <span style=color:#ae81ff>90.91069189446978</span>
</code></pre></div><p><img src=https://miro.medium.com/max/573/0*041BsubE6iLfLlkK.png alt=img></p><p>Congratulations on achieving a <strong>91.4%</strong> validation score in <strong>23 seconds</strong> on the whole dataset!</p><h3 id=boost-it-up>Boost it up!</h3><p>Now that we have our features all engineered and model tuned, we can turn the number of trees <strong>all the way up</strong> to get an accuracy of <strong>92%</strong> on the validation set.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>m <span style=color:#f92672>=</span> RandomForestRegressor(n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>160</span>, max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>,
          n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, oob_score<span style=color:#f92672>=</span>True)
<span style=color:#f92672>%</span>time m<span style=color:#f92672>.</span>fit(X_train, y_train)
print_score(m)

<span style=color:#f92672>----</span>
Output:

<span style=color:#f92672>&gt;</span> CPU times: user <span style=color:#ae81ff>5</span>min <span style=color:#ae81ff>52</span>s, sys: <span style=color:#ae81ff>11.4</span> s, total: <span style=color:#ae81ff>6</span>min <span style=color:#ae81ff>4</span>s
<span style=color:#f92672>&gt;</span> Wall time: <span style=color:#ae81ff>1</span>min <span style=color:#ae81ff>53</span>s
<span style=color:#f92672>&gt;</span> RMSE Train:  <span style=color:#ae81ff>8.013775394813543</span>
<span style=color:#f92672>&gt;</span> RMSE Valid:  <span style=color:#ae81ff>21.09451703745362</span>
<span style=color:#f92672>&gt;</span>  R2  Train:  <span style=color:#ae81ff>98.65782495931056</span>
<span style=color:#f92672>&gt;</span>  R2  Valid:  <span style=color:#ae81ff>92.05329397797038</span>
<span style=color:#f92672>&gt;</span>    OOB    :  <span style=color:#ae81ff>91.45711650192492</span>
</code></pre></div><p>This gets us into the <strong>top 1%</strong> of the competition and it is trained in under <strong>2 mins</strong>.</p><p><img src=https://miro.medium.com/max/247/0*lqg4n8qMeEUkVRmM.png alt=img></p><hr><p><em><strong>Link to <a href=https://www.sdhnshu.com/showcase/random-forests>Part 1</a></strong></em></p><h3 id=further-reading>Further reading</h3><ul><li><a href=https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-1-84a1dc2b5236>Fast.ai ML course notes</a></li><li><a href=https://medium.com/@ag.ds.bubble/model-interpretability-a4244d82ffb2>Model interpretability</a></li><li><a href=https://medium.com/usf-msds/intuitive-interpretation-of-random-forest-2238687cae45>Intuitive interpretation of random forests</a></li><li><a href=https://structuringtheunstructured.blogspot.com/2017/11/coloring-with-random-forests.html>Coloring with random forests</a></li><li><a href=https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80>Gradient Boosting vs Random Forest</a></li></ul></div></div></div></div></div></div></div><script src=https://www.sdhnshu.com/js/jquery.min.js></script><script src=https://www.sdhnshu.com/js/bootstrap.min.js></script><script src=https://www.sdhnshu.com/js/jquery.cookie.js></script><script src=https://www.sdhnshu.com/js/ekko-lightbox.js></script><script src=https://www.sdhnshu.com/js/jquery.scrollTo.min.js></script><script src=https://www.sdhnshu.com/js/masonry.pkgd.min.js></script><script src=https://www.sdhnshu.com/js/imagesloaded.pkgd.min.js></script><script src=https://www.sdhnshu.com/js/owl.carousel.min.js></script><script src=https://www.sdhnshu.com/js/front.js></script></body></html>